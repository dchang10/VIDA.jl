# How to use `DivIDE`

## Introduction
`DivIDE` is based on the idea of intrepreting the image as a probility distribution. Namely since any image is integrable, the space of images is in one-to-one correspondence with a probability distribution, especially since the total flux of the image is already known a priori.

Therefore, our idea is very close to variational inference. Namely, where we view the image as a distribution and we aim to find a approximation of the distribution given some parameteric family $f_\theta(x,y)$, which for our purproses we will typically call a *filter*. 

The choice of filter, depends on the problem of interest, namely what features we are interested in. Typically for the EHT where our images tend to be rings, we are interested in

 - Radius r₀
 - Width or half width σ
 - Structural asymmetry τ
 - Flux asymmetry s
 - Position angle ξ

`DivIDE` then defines a series of filters parameterize these features.

## filters
Currently we have 5 filters defined, although they all belong to the same family. For an example on how to see the process for defining your own filter please see the [readme](https://github.com/ptiede/DivIDE.jl/blob/master/README.md).

The filters implemented are:

 - `GaussianRing` which is a symmetric and circular Gaussian ring.
 - `SlashedGaussianRing` which is a circular Gaussian ring with a flux gradient across its emission.
 - `EllipticalGaussianRing` symmetric Gaussian elliptical ring, where the emission is constant across the ring, unlike with the SlashedGaussianRing.
 - `GeneralGaussianRing` A combination of the two above where the ring is allowed to be elliptical and have a intensity gradient.
 - `TIDAGaussianRing` The GeneralGaussianRing, but where the asymmetry and flux orienation are fixed relative to one another.

## Divergences
In order to extract features we first need a cost function that penalized our parameterized distributions $f_\theta(x,y)$. Since we are considering the image as a probability distribution, one cost function would be the distance or **divergence** between two distributions. A probability divergence is just a functional that takes in two probability distributions p,q and is minimized iff $p\equiv q$.

Currently we have two divergences implemented in `DivIDE`
 - Bhattacharya measure 
 ```math
 BH(f_\theta|I) = \int \sqrt{f_\theta(x,y)I(x,y)} dxdy.
 ```
 - KL divergence 
 ```math
 KL(f_\theta|I) = \int f_\theta(x,y)\log\left(\frac{f_\theta(x,y)}{I(x,y)}\right)dxdy. 
 ```
Both divergences give very similar answers, although we found the BH to be easier to maximize.


## Using `DivIDE`
Using DivIDE is based on constructing three items:
 1. Data, i.e. an image that you want to extract features from.
 2. Cost function, i.e. pick if you want to use the KL or BH divergence 
 3. Filter, i.e. construct the family of distributions or filters that you will use to approximate the image.
Then all you need to do is minimize the divergence and you will have extracted you image features.

### Step 1 Read in Data
`DivIDE` currently only works with fits images. THe fits header is based off of what [eht-imaging](https://github.com/achael/eht-imaging) outputs. So as long as you stick to that standard you should be fine.

```julia
using DivIDE
using PyPlot

#To read in an image we just call load_ehtimfits
img = load_ehtimfits(".julia/dev/DivIDE/example/data/elliptical_gaussian_rot-135.00.fits")
#Then because of Julia's multiple dispatch we can just plot using PyPlots
#plot command!
plot(img)
```
We can then easily create a divergence cost function to fit using the `make_div` function

```julia
#Makes the kl divergence
kl = make_div(img, :KL)
#Makes the bh divergence
bh = make_div(img, :Bh)
```
The specify the filter we then just need to create an object with the correct type. Julia will then dispatch on that type so that the correct function evaluation is used.
```julia
#Using a GaussianRing filter
```